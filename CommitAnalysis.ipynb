{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Commit Classification and Prediction — Problem Description\n",
        "\n",
        "Software repositories such as GitHub, Bitbucket, Gitlab etc. use commits to record changes in the source code, allowing teams of developers to collaborate effectively. Each commit includes a commit message, a brief summary describing the change. These messages generally shows whether the change involves fixing a bug, adding a feature, updating documentation, refactoring code, or making other non-functional improvements.\n",
        "\n",
        "Automatically analyzing these commit messages is highly valuable because software projects frequently suffer from information overload, where developers spend significant time understanding the nature of past changes, which helps in identifying bug-prone commits, and prioritizing maintenance tasks. Prior research shows that commit categorization can accelerate issue resolution, enhance code review efficiency, and support project management workflows.\n",
        "\n",
        "This project focuses on commit message classification, where the objective is to predict the macro-type of a software commit using only its natural-language commit message. We treat this as a supervised machine learning problem, leveraging a labeled dataset compiled from well-established and trusted sources. The dataset categorizes commits into five broad classes:\n",
        "\n",
        "- Corrective — commits related to bug fixes or fault correction\n",
        "\n",
        "- Feature — commits introducing new functionality or enhancements\n",
        "\n",
        "- Non-Functional — documentation updates, formatting, or other structural changes that do not alter behavior\n",
        "\n",
        "- Perfective — improvements to code quality, refactoring, or maintainability enhancements\n",
        "\n",
        "- Unknown — commits that are auto-generated or whose purpose cannot be clearly determined\n",
        "\n",
        "These categories provide a meaningful high-level taxonomy that supports automated analysis of software evolution and enables more efficient maintainability workflows.\n",
        "\n",
        "To address this prediction task, we evaluate both traditional machine learning techniques (TF-IDF combined with Logistic Regression, Naive Bayes, and Random Forest) and a modern deep learning model (BERT). By applying natural language processing methods, our aim is to identify the most effective model for commit classification and to highlight how automated commit analysis can significantly aid software maintenance, quality assurance, and project analytics."
      ],
      "metadata": {
        "id": "8jeQS9l4lDuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the packages"
      ],
      "metadata": {
        "id": "VaBfruuR1dv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate datasets sentencepiece"
      ],
      "metadata": {
        "id": "_q9v8L1mfCQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the libraries"
      ],
      "metadata": {
        "id": "7GL11HZt1kdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "import os\n",
        "\n",
        "\n",
        "from datasets import load_dataset, Dataset\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    ConfusionMatrixDisplay\n",
        ")\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")"
      ],
      "metadata": {
        "id": "ErItTu9yfeqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable Weights & Biases logging\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "Ts9WYjAzgOyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download English Stopwords\n",
        "We download English stopwords because they are common words like the, is, and, of that usually do not carry meaningful information for text classification.\n",
        "\n",
        "By removing stopwords during preprocessing, we:\n",
        "- Reduce noise in the text,\n",
        "- Focus the model on meaningful words, and\n",
        "- Improve model performance and efficiency."
      ],
      "metadata": {
        "id": "SAMy5OlosA-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the NLTK English stopwords list (common words to remove during text preprocessing)\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# Load the English stopwords into a set for faster lookup during text cleaning\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "mMYELpKUhajg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check device GPU or CPU\n",
        "print(\"Torch device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "JmLk9DQssJQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the dataset"
      ],
      "metadata": {
        "id": "uu3ziwkJhgvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"0x404/ccs_dataset\")"
      ],
      "metadata": {
        "id": "PhpJdXftheZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert each split to Pandas DataFrame\n",
        "train_df = dataset[\"train\"].to_pandas()\n",
        "train_df"
      ],
      "metadata": {
        "id": "-XQ_h7rMT0Yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns"
      ],
      "metadata": {
        "id": "tBhxg66lm2AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df   = dataset[\"eval\"].to_pandas()\n",
        "val_df"
      ],
      "metadata": {
        "id": "MNLDT7e4UCLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df  = dataset[\"test\"].to_pandas()\n",
        "test_df"
      ],
      "metadata": {
        "id": "1dPj8C-xUD4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nDATASET SIZES:\")\n",
        "for split in dataset.keys():\n",
        "    print(f\"{split}: {len(dataset[split])} samples\")"
      ],
      "metadata": {
        "id": "0AY2ja2ThpDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to Pandas DataFrames and rename columns\n",
        "train_df_raw = dataset[\"train\"].to_pandas()[[\"masked_commit_message\", \"annotated_type\"]]\n",
        "val_df_raw   = dataset[\"eval\"].to_pandas()[[\"masked_commit_message\", \"annotated_type\"]]\n",
        "test_df_raw  = dataset[\"test\"].to_pandas()[[\"masked_commit_message\", \"annotated_type\"]]\n",
        "\n",
        "train_df_raw = train_df_raw.rename(columns={\"masked_commit_message\": \"Message\", \"annotated_type\": \"Ground truth\"})\n",
        "val_df_raw   = val_df_raw.rename(columns={\"masked_commit_message\": \"Message\", \"annotated_type\": \"Ground truth\"})\n",
        "test_df_raw  = test_df_raw.rename(columns={\"masked_commit_message\": \"Message\", \"annotated_type\": \"Ground truth\"})"
      ],
      "metadata": {
        "id": "xD6B7-NRsSHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Raw training data (First 5 rows)\")\n",
        "train_df_raw.head()"
      ],
      "metadata": {
        "id": "yPEAr3TOsZLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean an dPreprocess Commit Messages\n",
        "\n",
        "\n",
        "This function cleans commit messages by converting to lowercase, removing URLs,\n",
        "keeping only alphabetic characters, removing extra spaces, and filtering out stopwords.\n"
      ],
      "metadata": {
        "id": "mXLzdT0pssOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(df):\n",
        "    \"\"\"\n",
        "    Clean commit messages:\n",
        "    - Convert to lowercase\n",
        "    - Remove URLs\n",
        "    - Keep only alphabetic characters\n",
        "    - Remove stopwords\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"Message\"] = df[\"Message\"].astype(str)\n",
        "\n",
        "    def clean(t):\n",
        "        t = t.lower()\n",
        "        t = re.sub(r\"http\\S+\", \"\", t)          # Remove URLs\n",
        "        t = re.sub(r\"[^a-z\\s]\", \" \", t)       # Remove non-alphabetic characters\n",
        "        t = re.sub(r\"\\s+\", \" \", t).strip()    # Remove extra spaces\n",
        "        return \" \".join([w for w in t.split() if w not in stop_words])\n",
        "\n",
        "    df[\"clean_message\"] = df[\"Message\"].apply(clean)\n",
        "    return df"
      ],
      "metadata": {
        "id": "3REeoQQEsk8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = preprocess(train_df_raw)\n",
        "val_df   = preprocess(val_df_raw)\n",
        "test_df  = preprocess(test_df_raw)"
      ],
      "metadata": {
        "id": "vX-1G-lds7p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Cleaned Training Data (First 5 rows)\")\n",
        "\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "vUUiTQGds-A8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explorartory Data Analysis"
      ],
      "metadata": {
        "id": "L4ff5sGL2Db4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label Distribution Plot\n",
        "Shows how frequently each ground-truth label appears in the training set, helping identify class imbalance."
      ],
      "metadata": {
        "id": "ymaHVTsTtZQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.countplot(y=train_df[\"Ground truth\"], order=train_df[\"Ground truth\"].value_counts().index)\n",
        "plt.title(\"Label Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EQMgd5ZDtTVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"msg_len\"] = train_df[\"clean_message\"].apply(lambda x: len(x.split()))\n",
        "train_df"
      ],
      "metadata": {
        "id": "b69j2oTitc8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Message Length Distribution\n",
        "Displays how long commit messages typically are, revealing patterns such as very short or very long messages"
      ],
      "metadata": {
        "id": "9ENtcGiLtlCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(train_df[\"msg_len\"], bins=40)\n",
        "plt.title(\"Message Length Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OvH4-vrVtiOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. TF-IDF Feature Extraction\n",
        "Converts text into numerical vectors by measuring how important each word is within a message relative to the entire dataset. This helps machine-learning models understand and compare commit messages based on their content.\n",
        "\n"
      ],
      "metadata": {
        "id": "n-fi-dSfuGF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train = vectorizer.fit_transform(train_df[\"clean_message\"])\n",
        "X_val   = vectorizer.transform(val_df[\"clean_message\"])\n",
        "X_test  = vectorizer.transform(test_df[\"clean_message\"])\n",
        "\n",
        "y_train = train_df[\"Ground truth\"]\n",
        "y_val   = val_df[\"Ground truth\"]\n",
        "y_test  = test_df[\"Ground truth\"]"
      ],
      "metadata": {
        "id": "X1jziXmduBoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Classical Machine Learning Models"
      ],
      "metadata": {
        "id": "fhEx3m5AuN__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression Model\n",
        "A linear classification model that learns weighted features to predict labels; class_weight=\"balanced\" helps handle class imbalance."
      ],
      "metadata": {
        "id": "UoFDcB482o_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "log = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
        "log.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "NX_UByAmuY4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes Model\n",
        "A probabilistic classifier based on word-frequency statistics, assuming features are conditionally independent—fast and effective for text."
      ],
      "metadata": {
        "id": "WwrVgGfe2vwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive bayes\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "d3dlaW6mudfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest\n",
        "An ensemble of many decision trees that vote on the final prediction, improving accuracy and reducing overfitting."
      ],
      "metadata": {
        "id": "ZS8SOqGi24Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=300, random_state=42)\n",
        "rf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "GgyuOlVJur_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importances from Random Forest\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Get feature names from TF-IDF vectorizer\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Get indices of top 15 important features\n",
        "indices = np.argsort(importances)[-10:][::-1]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.barplot(x=importances[indices], y=np.array(feature_names)[indices], palette=\"magma\")\n",
        "plt.title(\"Top 10 Random Forest TF-IDF Features\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QRgo_lYqYxec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Of TF-IDF(Term Frequency–Inverse Document Frequency) Models\n",
        "Each trained model is evaluated using accuracy and macro-F1 scores on the training, validation, and test sets. This helps compare how well the models learn from the data, generalize to unseen data, and handle class imbalance. The resulting table summarizes the performance of Logistic Regression, Naive Bayes, and Random Forest side-by-side."
      ],
      "metadata": {
        "id": "l54TIiTE3BiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(name, model, X_train, X_val, X_test, y_train, y_val, y_test):\n",
        "    return {\n",
        "        \"Model\": name,\n",
        "        \"Train Acc\": accuracy_score(y_train, model.predict(X_train)),\n",
        "        \"Val Acc\": accuracy_score(y_val, model.predict(X_val)),\n",
        "        \"Test Acc\": accuracy_score(y_test, model.predict(X_test)),\n",
        "        \"Train F1\": f1_score(y_train, model.predict(X_train), average=\"macro\"),\n",
        "        \"Val F1\": f1_score(y_val, model.predict(X_val), average=\"macro\"),\n",
        "        \"Test F1\": f1_score(y_test, model.predict(X_test), average=\"macro\"),\n",
        "    }"
      ],
      "metadata": {
        "id": "ZoWW0tGhuuwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_detailed = [\n",
        "    evaluate_model(\"Logistic Regression\", log, X_train, X_val, X_test, y_train, y_val, y_test),\n",
        "    evaluate_model(\"Naive Bayes\", nb, X_train, X_val, X_test, y_train, y_val, y_test),\n",
        "    evaluate_model(\"Random Forest\", rf, X_train, X_val, X_test, y_train, y_val, y_test),\n",
        "]\n",
        "\n",
        "df_compare = pd.DataFrame(results_detailed)\n",
        "print(\"\\n Training / Validation / Test data performance results (TF-IDF Models) \")\n",
        "df_compare"
      ],
      "metadata": {
        "id": "IYnRSm_AuzMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = sorted(train_df[\"Ground truth\"].unique())"
      ],
      "metadata": {
        "id": "bd2CI48ivCn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix\n",
        "A confusion matrix is used to visualize how well a classification model performs by showing the counts of correct and incorrect predictions for each class.\n",
        "It helps identify which classes the model predicts accurately and where it makes mistakes."
      ],
      "metadata": {
        "id": "_RZ54HRl30N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, title, cmap):\n",
        "    plt.figure(figsize=(10,7))\n",
        "    sns.heatmap(\n",
        "        confusion_matrix(y_true, y_pred, labels=labels),\n",
        "        annot=True,\n",
        "        cmap=cmap,\n",
        "        fmt=\"d\",\n",
        "        xticklabels=labels,\n",
        "        yticklabels=labels\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Yb-qObUivE72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_test, log.predict(X_test), \"Logistic Regression — Confusion Matrix\", \"Blues\")"
      ],
      "metadata": {
        "id": "iu74CPjzvH70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_test, nb.predict(X_test),  \"Naive Bayes — Confusion Matrix\", \"Oranges\")"
      ],
      "metadata": {
        "id": "YeJ7-mQ5vM8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Naive Bayes model is performing well overall, with strong correct predictions for classes like build, ci, docs, perf, style, and test, but it notably confuses semantically similar categories such as feat, fix, refactor"
      ],
      "metadata": {
        "id": "oega5sWt0pS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_test, rf.predict(X_test),  \"Random Forest — Confusion Matrix\", \"Greens\")"
      ],
      "metadata": {
        "id": "uDssAMPUvOhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data for BERT(Bidirectional Encoder Representations from Transformers)\n",
        "It is a powerful pre-trained language model developed by Google that understands text by looking at words in both directions (left and right), making it highly effective for NLP tasks like classification, sentiment analysis, and question answering. BERT can deeply understand the meaning, context, and intent behind commit messages. This helps classify commits more accurately (e.g., bug fix, feature, refactor) because BERT captures nuances such as technical terms, action verbs, and contextual relationships that simpler models (TF-IDF + ML) may miss.\n",
        "\n",
        "Implementation Explanation: This code prepares commit messages for BERT-based classification. It converts textual labels to numeric IDs, formats the data as HuggingFace Dataset objects, tokenizes the messages into BERT-compatible input IDs and attention masks, removes the original text, and sets the datasets to PyTorch format for model training. This ensures the data is ready for fine-tuning a BERT model."
      ],
      "metadata": {
        "id": "ENHVu8j3vpY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = labels\n",
        "label_list"
      ],
      "metadata": {
        "id": "XRe6Xw3CvSVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create label-to-ID and ID-to-label mappings\n",
        "\n",
        "label2id = {l:i for i,l in enumerate(label_list)}\n",
        "id2label = {i:l for l,i in label2id.items()}\n",
        "id2label"
      ],
      "metadata": {
        "id": "tddnpr9dvuOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert original dataframe into a format suitable for BERT\n",
        "\n",
        "def make_bert_df(df):\n",
        "    x = df[[\"clean_message\", \"Ground truth\"]].copy()\n",
        "    x = x.rename(columns={\"clean_message\": \"text\", \"Ground truth\": \"label\"})\n",
        "    x[\"label\"] = x[\"label\"].map(label2id)\n",
        "    return x"
      ],
      "metadata": {
        "id": "XqPKj8k-vtDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DataFrames to HuggingFace Dataset objects\n",
        "\n",
        "train_bert = Dataset.from_pandas(make_bert_df(train_df))\n",
        "val_bert   = Dataset.from_pandas(make_bert_df(val_df))\n",
        "test_bert  = Dataset.from_pandas(make_bert_df(test_df))"
      ],
      "metadata": {
        "id": "dXLlAZG7v3LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BERT tokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "IgcBuEIjv6nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization function to convert text → BERT input IDs & attention masks\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,            # Cut text longer than max_length\n",
        "        padding=\"max_length\",       # Pad all sequences to a fixed length\n",
        "        max_length=128              # Maximum token length\n",
        "        )"
      ],
      "metadata": {
        "id": "xT-Y95dhv9XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize datasets and remove raw text column (BERT uses tokenized inputs instead)\n",
        "\n",
        "train_tok = train_bert.map(tokenize, batched=True).remove_columns([\"text\"])\n",
        "val_tok   = val_bert.map(tokenize, batched=True).remove_columns([\"text\"])\n",
        "test_tok  = test_bert.map(tokenize, batched=True).remove_columns([\"text\"])"
      ],
      "metadata": {
        "id": "njzjYZq5wAWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tokenized datasets into PyTorch format for training\n",
        "\n",
        "train_tok.set_format(\"torch\")\n",
        "val_tok.set_format(\"torch\")\n",
        "test_tok.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "HnjBGqSTwCw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Model Training"
      ],
      "metadata": {
        "id": "CWlU_pDxwF21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT model for sequence classification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")"
      ],
      "metadata": {
        "id": "SDK33M3-5Vad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training hyperparameters and settings\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"bert_ccs\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "ZotkL4J35aEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define evaluation metrics function\n",
        "\n",
        "def metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(p.label_ids, preds),\n",
        "        \"f1_macro\": f1_score(p.label_ids, preds, average=\"macro\")\n",
        "    }"
      ],
      "metadata": {
        "id": "QiikbL5-5iHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=val_tok,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "0We_i2fwwXqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "MBc0am-D5rBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BERT model achieved a Test Accuracy of 57.5% and F1-score of 56.5%, demonstrating its capability to capture semantic patterns in commit messages. Its main difficulty was distinguishing commit types with overlapping meanings, such as 'feature addition' vs. 'code refactoring"
      ],
      "metadata": {
        "id": "imcKldTdyrtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_history = trainer.state.log_history\n",
        "metrics_history"
      ],
      "metadata": {
        "id": "H3dn-ingzqmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training / Validation Metrics Over Epochs"
      ],
      "metadata": {
        "id": "grLNTWnGz6cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only keep entries with evaluation metrics\n",
        "eval_metrics = [m for m in metrics_history if \"eval_accuracy\" in m]\n",
        "\n",
        "epochs = [m[\"epoch\"] for m in eval_metrics]\n",
        "val_acc = [m[\"eval_accuracy\"] for m in eval_metrics]\n",
        "val_f1 = [m[\"eval_f1_macro\"] for m in eval_metrics]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(epochs, val_acc, label=\"Validation Accuracy\", marker='o')\n",
        "plt.plot(epochs, val_f1, label=\"Validation F1 Macro\", marker='x')\n",
        "plt.title(\"Validation Metrics Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dBAdBzOYzuXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label Distribution Visualization"
      ],
      "metadata": {
        "id": "FVM5srujz996"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(y=train_df[\"Ground truth\"], order=train_df[\"Ground truth\"].value_counts().index)\n",
        "plt.title(\"Label Distribution in Training Set\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xCNvCHCiz-sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction Distribution"
      ],
      "metadata": {
        "id": "VCvFVgEK0DNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preds = trainer.predict(test_tok)\n",
        "bert_pred = np.argmax(bert_preds.predictions, axis=1)\n",
        "bert_true = bert_preds.label_ids\n",
        "\n",
        "print(\"\\nBERT Classification Report:\\n\",\n",
        "      classification_report(bert_true, bert_pred, target_names=label_list))"
      ],
      "metadata": {
        "id": "G5EnBEgPwgx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(y=pd.Series(bert_pred).map(lambda x: id2label[x]))\n",
        "plt.title(\"Prediction Distribution on Test Set\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H4P-0jeH0EAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Testing Evaluation"
      ],
      "metadata": {
        "id": "TeYReFEhwdnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Misclassifications\n",
        "As To understand errors, you can show a few commit messages that were misclassified:"
      ],
      "metadata": {
        "id": "1nSBgL610JmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "misclassified = test_df.copy()\n",
        "misclassified['pred'] = bert_pred\n",
        "misclassified = misclassified[misclassified['pred'] != misclassified['Ground truth'].map(label2id)]\n",
        "misclassified[['Message', 'Ground truth', 'pred']].head(10)"
      ],
      "metadata": {
        "id": "gUTbSyRo0Ijz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "cm_bert = confusion_matrix(bert_true, bert_pred)\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(cm_bert, annot=True, cmap=\"Purples\",\n",
        "            xticklabels=label_list, yticklabels=label_list)\n",
        "plt.title(\"BERT Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n4URiQ7dw3do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalized Confusion Matrix\n",
        "def plot_normalized_confusion_matrix(y_true, y_pred, title):\n",
        "    labels = np.arange(len(label_list))  # numeric labels 0 to 9\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels, normalize='true')\n",
        "\n",
        "    plt.figure(figsize=(10,7))\n",
        "    sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Purples\",\n",
        "                xticklabels=label_list, yticklabels=label_list)\n",
        "    plt.title(title)\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7Bj1ZH86Xsuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_normalized_confusion_matrix(bert_true, bert_pred, \"BERT Normalized Confusion Matrix\")"
      ],
      "metadata": {
        "id": "4MU5n-yxXxdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Training / Validation / Test performance\n"
      ],
      "metadata": {
        "id": "Yzae3LP27N3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_out = trainer.predict(train_tok)\n",
        "bert_train_pred = np.argmax(train_out.predictions, axis=1)\n",
        "bert_train_true = train_out.label_ids\n",
        "\n",
        "val_out = trainer.predict(val_tok)\n",
        "bert_val_pred = np.argmax(val_out.predictions, axis=1)\n",
        "bert_val_true = val_out.label_ids\n",
        "\n",
        "bert_results = {\n",
        "    \"Model\": \"BERT\",\n",
        "    \"Train Acc\": accuracy_score(bert_train_true, bert_train_pred),\n",
        "    \"Val Acc\": accuracy_score(bert_val_true, bert_val_pred),\n",
        "    \"Test Acc\": accuracy_score(bert_true, bert_pred),\n",
        "    \"Train F1\": f1_score(bert_train_true, bert_train_pred, average=\"macro\"),\n",
        "    \"Val F1\": f1_score(bert_val_true, bert_val_pred, average=\"macro\"),\n",
        "    \"Test F1\": f1_score(bert_true, bert_pred, average=\"macro\"),\n",
        "}\n",
        "\n",
        "print(\"\\n=== BERT TRAIN / VAL / TEST PERFORMANCE ===\")\n",
        "print(pd.DataFrame([bert_results]))"
      ],
      "metadata": {
        "id": "yYMJ8cypw7yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observations\n",
        "Overall, TF‑IDF + Logistic Regression gives the best balance of performance, simplicity, and interpretability on your commit-message task, and the notebook would benefit most from making label mappings explicit, emphasizing macro‑F1 and per-class metrics, and adding a concise, side‑by‑side comparison and brief error analysis (especially versus BERT) to clearly show where each model helps or struggles"
      ],
      "metadata": {
        "id": "qpmoug8a2ZnW"
      }
    }
  ]
}